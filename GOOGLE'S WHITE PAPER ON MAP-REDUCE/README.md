The paper that started it all.

Before reading the paper, this <a href="https://www.youtube.com/watch?v=AZovvBgRLIY">video</a> provides a quick intro.

So once you watch the video and read the paper you should be able to get a good idea about hadoop and the infamous MapReduce.

So take the example of a food factory. Lets say a truck arrives at the factory with some raw materials and they get dispatched on to a group of conveyer belts where workers are processing the materials and making some different eatable spheres and then dropping them in large containers one for each type of sphere. Then once containers are full, they are moved to a packaging area where say 9 of the same colored spheres are packed together and sent on a conveyer belt for further packing. Now say 20 of these packets are being put in a box and these boxes are being loaded into a truck for dispatch. 

Now in this example the first part of the process where the spheres are seperated based on their color can be imagined as the Map part and the packaging part of these spheres can be imagined as the Reduce part. The idea is to develop this example as the specialization progresses and understand the evolution of the hadoop ecosystem.
